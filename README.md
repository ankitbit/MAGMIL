# Model Agnostic Methods for Interpretable Machine Learning

European Union’s new General Data Protection Regulation which is going to be enforced beginning
from 25th of May, 2018 will have potential impact on the routine use of machine learning algorithms
by restricting automated individual decision-making (that is, algorithms that make decisions based on
user-level predictors) which ”significantly affect” users. The law will also effectively create a ”right to
explanation,” whereby a user can ask for an explanation of an algorithmic decision that was made about
them.
Considering such challenging norms on the use of machine learning systems, we are making an attempt
to make the models more interpretable. While we are concerned about developing a deeper understanding
of decisions made by a machine learning model, the idea of extracting the explaintations from the machine
learning system, also known as model-agnostic interpretability methods has some benefits over techniques
such as model specific interpretability methods in terms of flexibility. Similarly, the idea of using only
interpetable models has some drawbacks in terms of accuracy. In this study, we will try to study the
application of model agnostic interpretability methods for making highly complex black box methods
more interpretable. Some aspects of the model agnostic explanation systems such as model flexibility,
explanation flexibility and representation flexibility will also be examined.

## References-
- [Practical Techniques for Interpreting Machine Learning Models:
Introductory Open Source Examples Using Python, H2O, and XGBoost](https://fatconference.org/static/tutorials/hall_interpretable18.pdf)

- [Machine Learning Interpretability](https://github.com/h2oai/mli-resources#dockerfile)

- [A Unified Approach to Interpreting Model Predictions](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf)
- [Anchors- High Precision Model Agnostic Explanations](https://homes.cs.washington.edu/~marcotcr/aaai18.pdf)
