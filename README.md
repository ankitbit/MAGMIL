# MAGM_IML
European Union’s new General Data Protection Regulation which is going to be enforced beginning from 25th of May,  2018 will have potential impact on the routine use of machine learning algorithms by restricting automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which ”significantly affect” users.  The law will also effectively create a ”right to explanation,” whereby a user can ask for an explanation of an algorithmic decision that was made about them.

Considering such challenging norms on the use of machine learning systems, we are making an attempt to make the models more interpretable. While we are concerned about developing a deeper understanding of decisions made by a machine learning model, the idea of extracting the explaintations from the machine learning system, also known as model-agnostic interpretability methods has some benefits over techniques such as model specific interpretability methods in terms of flexibility.  Similarly, the idea of using only interpetable models has some drawbacks in terms of accuracy.  In this study, we will try to study the application of model agnostic interpretability methods for making highly complex black box methods more interpretable.  Some aspects of the model agnostic explanation systems such as model flexibility, explanation flexibility and representation flexibility will also be examined.
